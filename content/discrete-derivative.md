+++
title = "Matrix Derivation: What if the derivative, but matrices?"
date = 2025-07-14

[extra]
subtitle = "5 min read. Published TODO."
tab_title = "Matrix Derivation"
+++

The derivative is an operator that takes a continuous function $f:\mathbb{C}\to\mathbb{C}$ to a continuous function $f':\mathbb{C}\to\mathbb{C}$. It has many generalizations, for example the gradient (when $f$ has multiple inputs), the vector derivative (when $f$ has multiple outputs), or even the [finite difference operator](https://en.wikipedia.org/wiki/Finite_difference), defined as $\Delta f(x)=f(x+1)-f(x)$. But, like, what if we could generalize the derivative to something that wasn't even a function?

Instead of taking a function to a function, let's make an operator that takes a matrix to another matrix. For simplicity, we will only be considering matrices with complex components here. Because of this, instead of using the regular matrix transpose $A^T$ we will instead be using the [conjugate transpose](https://en.wikipedia.org/wiki/Conjugate_transpose) $A^\dag$, which is the regular transpose except every element is now its complex conjugate.

A very fundamental rule of the derivative is the product rule, so let's define a Matrix [Derivation](https://en.wikipedia.org/wiki/Derivation_(differential_algebra)) to have an analogous version of the product rule. Our Matrix Derivation $\mathbb{D}$ will have a few properties:
- Size preservation: Given an $n\times m$ matrix $A$, $\mathbb{D}(A)$ is also an $n\times m$ matrix.
- Linearity: $\mathbb{D}(aA+bB)=a\mathbb{D}(A)+b\mathbb{D}(B)$ for scalars $a,b$ and matrices of equal size $A,B$.
- The Product Rule: $\mathbb{D}(AB)=A\mathbb{D}(B)+\mathbb{D}(A)B$ for matrix $A\in\mathbb{C}^{n\times k}$ and $B\in\mathbb{C}^{k\times m}$.

I first claim that I claim that the matrix derivation of a $1\times 1$ matrix is always $0$. Given $a\in\mathbb{C}^{1\times 1}$, because $\mathbb{D}$ is linear, $\mathbb{D}(a)=da$ for some scalar $d$. However, $dab = \mathbb{D}(ab)=aD(b)+D(a)b=dab + dab=2dab$, so $d$ must be $0$.

Now let's look at row and column vectors. Because $\mathbb{D}$ is linear, given column vector $v_{(n)}\in\mathbb{C}^{n\times 1}$, $\mathbb{D}(v_{(n)})=D_{(n)}v_{(n)}$ for some square matrix $D_{(n)}:\mathbb{C}^{n\times n}$. This is also true for $1\times n$ row vectors, such that for row vectors $u_{(n)}^\dag $ and square transformation matrix $E_{(n)}:\mathbb{C}^{n\times n}$, $D(u_{(n)}^\dag )=u_{(n)}^\dag E_{(n)}$. Now, because the dot product of two column vectors is a $1\times 1$ matrix, which has matrix derivation $0$, we should expect that $\mathbb{D}(u_{(n)}^\dag v_{(n)})=0$. Applying the product rule, $\mathbb{D}(u_{(n)}^\dag v_{(n)})=u_{(n)}^\dag \mathbb{D}(v_{(n)})+\mathbb{D}(u_{(n)}^\dag )v_{(n)}=u_{(n)}^\dag D_{(n)}v_{(n)}+u_{(n)}^\dag E_{(n)}v_{(n)}=u_{(n)}^\dag \left(D_{(n)}+E_{(n)}\right)v_{(n)}=0$. Because this is true for all $u_{(n)}^\dag $ and $v_{(n)}$, we know that $D_{(n)}+E_{(n)}=0$ which means that $E_{(n)}=-D_{(n)}$. Plugging this into our formula for the matrix derivation of a row vector, we get that $\mathbb{D}(u_{(n)}^\dag)=-u_{(n)}^\dag D_{(n)}$. Note that $D_{(1)}$ always equals zero because the matrix derivation of a $1\times 1$ matrix is always zero.

Finally, lets take a look at matrices. Let $\hat{e}_{(ni)}$ be the $i$-th unit vector in $\mathbb{C}^n$. In this way, if matrix $A:\mathbb{C}^{n\times m}$ has components $A_{ij}$, then $A=\sum_{i=1}^{n}\sum_{j=1}^m A_{ij}\hat{e}_{(ni)}\hat{e}_{(mj)}^\dag$. We can use this to find the matrix derivation of $A$. By linearity, $\mathbb{D}(A)=\sum_{i=1}^{n}\sum_{j=1}^m A_{ij}\mathbb{D}\left(\hat{e}_{(ni)}\hat{e}_{(mj)}^\dag\right)$. By the product rule, this equals $\sum_{i=1}^{n}\sum_{j=1}^m A_{ij}\left(\mathbb{D}\left(\hat{e}_{(ni)}\right)\hat{e}_{(mj)}^\dag+\hat{e}_{(ni)}\mathbb{D}\left(\hat{e}_{(mj)}^\dag\right)\right)$. By the definitions above, this then equals $\sum_{i=1}^{n}\sum_{j=1}^m A_{ij}\left(D_{(n)}\hat{e}_{(ni)}\hat{e}_{(mj)}^\dag-\hat{e}_{(ni)}\hat{e}_{(mj)}^\dag D_{(m)}\right)$. And simplifying, we get $D_{(n)}\sum_{i=1}^{n}\sum_{j=1}^m A_{ij}\hat{e}_{(ni)}\hat{e}_{(mj)}^\dag-\sum_{i=1}^{n}\sum_{j=1}^m A_{ij}\hat{e}_{(ni)}\hat{e}_{(mj)}^\dag D_{(m)}=D_{(n)}A-AD_{(m)}$. Isn't that a nice formula? Let's quickly check that this satisfies the product rule. Given $A\in\mathbb{C}^{n\times k}$ and $B\in\mathbb{C}^{k\times m}$, we should get that $\mathbb{D}(AB)=D_{(n)}AB-ABD_{(m)}$. Using the product rule, we get that $\mathbb{D}(AB)=A\mathbb{D}(B)+\mathbb{D}(A)B=A\left(D_{(k)}B-BD_{(m)}\right)+\left(D_{(n)}A-AD_{(k)}\right)B=D_{(n)}AB-AD_{(k)}B+AD_{(k)}B-ABD_{(m)}=D_{(n)}AB-ABD_{(m)}$, which is exactly what we expected. From here we can also compute that if we apply the matrix derivation $k$ times, we get $\mathbb{D}^k(A)=\sum_{i=0}^k D_{(n)}^i A D_{(m)}^{k-i}$.

At this point given any sequence of square matrices $D_{(2)}, D_{(3)}, \ldots$, we can compute a matrix derivation based on those matrices. However, how are we ever going to pick what those should be? For each $D_{(n)}$, we would need to pick $n^2$ complex numbers to fully define it, which is a lot. I see two ways of narrowing down the set of possibilities.
- The first idea for a constraint is that each $D_{(n)}$ is [Hermitian](https://en.wikipedia.org/wiki/Hermitian_matrix), which means that $D_{(n)}^\dag=D_{(n)}$. This reduces the number of variables to configure by about half. However, the main interesting this about this is because $D_{(n)}$ can be diagonalized such that $D_{(n)}=P_{(n)}\Lambda_{(n)}P_{(n)}^{-1}$ for invertible matrix $P_{(n)}$ and real diagonal matrix $\Lambda_{(n)}$. What this would mean is that the $k$-th matrix derivation is $\mathbb{D}^{k}(A)=P_{(n)}\left(\sum_{i=0}^k \Lambda_{(n)}^i \left(P_{(n)}^{-1}AP_{(m)}\right) \Lambda_{(m)}^{k-i}\right)P_{(m)}^{-1}$, which if the diagonalization of $D_{(n)}$ and $D_{(m)}$ can be computed, makes finding the $k$-th matrix derivation very easy.
- The second idea for a constraint is that the matrix derivation of any square permutation matrix $S$ is zero. This would mean that $\mathbb{D}(SA)=S\mathbb{D}(A)$ and $\mathbb{D}(AS)=\mathbb{D}(A)S$, which creates symmetry over our choice of indices. This means that for the $n\times n$ permutation matrices $S_{(n)}$, we have $D_{(n)}S_{(n)}-S_{(n)}D_{(n)}=0\implies D_{(n)}=S_{(n)}D_{(n)}S_{(n)}$, which means that each $D_{(n)}$ is symmetric about the swap of two dimensions, which then means for a single $D_{(n)}$ all the off-diagonal components are equal, and all the on-diagonal components are equal. This at least takes the $n^2$ degrees of freedom down to $2$ for each $D_{(n)}$. If all the components are real, then this would also satisfy the above condition.

## Tensor Derivation: The brink of insanity
An $n\times m$ matrix is a collection of numbers with two indices, one for the row number with size $n$ and one for the column number with size $m$. In general, tensors are a generalization of matrices that can have any number of indices, each with possibly their own sizes. Tensors have two kinds of indices, "contravariant" indices and "covariant" indices. The contravariant indices form superscripts of the tensor, and the covariant indices form subscripts of the tensor. For example, $T^{ab}_{cde}$ is a tensor with two contravariant indices $a,b$, and three covariant indices $c,d,e$. The reason for these names comes from how tensors are used in physics, but we will ignore that for now.

Given a tensor, we can contract the tensor along a pair of contravariant and covariant indices of equal size $n$ like so: $T'^{a}_{ce}=\sum_{b=1}^{n} T^{ab}_{cbe}$. For concision, we use what is called Einstein summation notation and just write $T'^{a}_{ce}=T^{ab}_{cbe}$, where because $b$ is repeated it is assumed that it is summed over. One way of describing matrix multiplication $C'=AB$ is that $A$ and $B$ are each tensors with one contravariant and one covariant index, and first we product then together to get $C^{il}_{kj}=A^i_k B^l_j$, which has two contravariant and two covariant indices. Then, we contract $C$ over $l$ and $k$ to get $C'^i_j=C^{ik}_{kj}=A^i_k B^k_j=\sum_{k=1}^n A^i_k B^k_j$.

So yeah obviously we're going to figure our how to do Tensor Derivation now, which will satisfy the following rules:
- Size preservation: Given an $n\times m$ matrix $A$, $\mathbb{D}(A)$ is also an $n\times m$ matrix.
- Linearity: $\mathbb{D}(aA+bB)=a\mathbb{D}(A)+b\mathbb{D}(B)$ for scalars $a,b$ and matrices of equal size $A,B$.
- The Product Rule: $\mathbb{D}(AB)=A\mathbb{D}(B)+\mathbb{D}(A)B$ for matrix $A\in\mathbb{C}^{n\times k}$ and $B\in\mathbb{C}^{k\times m}$.
