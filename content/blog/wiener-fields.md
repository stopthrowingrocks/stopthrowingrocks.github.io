+++
title = "Modeling random fields"
date = 2025-06-25
+++

- TODO: Fix LaTeX rendering on website. Get tooltip working as well.

# Generalizing Brownian motion: Modeling random fields over arbitrary geometries

_TODO min read._

Hello! Thank you for coming here to read my article. This is part story, part walkthrough of TODO. It is intended to be read by people with at least about basic college-level math experience. Be prepared to take the time to parse and understand some potentially lengthy mathematical expressions to get the most out of it.

## Motivation: Inspiration from quantum gravity

I love theoretical physics. I've been subscribed to PBS Spacetime for about 8 years now and get excited when I see new things from Quanta Magazine. Theoretical physics is, in part, the search for a so-called Theory of Everything. Such a Theory of Everything would be able to explain every phenomenon and predict every outcome, at least in theory. It might take a very big computer and some clever approximations to actually calculate what the theory says, but at least you know what the rules of the universe are at their most fundamental level.

However, right now theoretical physicists are stuck with two successful but incompatible theories: Quantum Field Theory (QFT) and General Relativity (GR). QFT is best at explaining things that happen around the scale of atoms. GR is best at explaining gravity, which happens around heavy things like planets. By making informed simplifications of the two theories, they can also be extended to explain things that happen in our everyday life. As water drips from the faucet, the surface tension of the water is a quantum property, whereas the water falling into the sink is because of gravity.

And yet despite their success, QFT and GR remain separate theories. This is frustrating for theoretical physicists, because it means there are some phenomena that even in principle we don't know how to predict. Most of these have to do with black holes, like what happens inside a black hole, or on the surface of a black hole, or what happens if you look at reality at such a small scale that tiny black holes should appear randomly all the time.

A theory that would successfully unify QFT and GR would be a theory of Quantum Gravity. This is currently considered the Holy Grail of theoretical physics. It's also fascinating to me, and I like to think about what a theory of Quantum Gravity could look like.

Maybe part of Quantum Gravity is that space-time itself is somehow randomly configured. I got this idea in trying to some Quanta Magazine article iirc. But what could that even mean? What would that look like? I needed to get my hands on some math, so I decided to do this project.

## Prior work: Stochastic processes and Brownian motion

I decided to focus on trying to model some kind of randomly-fluctuating variable on an existing static space, like 2D or 3D space. It just so happens that there are a lot of tools for describing randomly-fluctuating varibles in time, under the name [Stochastic Process Theory](https://en.wikipedia.org/wiki/Stochastic_process).

The most basic example of a stochastic process like this is Brownian motion. The basic idea is that you have a random variable that at every timestep randomly increases or decreases a little bit. This could be used to model a stock price going up and down with time, or a continuous random walk in one dimension. For concision, let's call the units of Brownian motion "field", represented by $\phi$.

Brownian motion has many interesting properties.
- Brownian motion is not any specific shape or function, but rather a probabilistic collection of functions that obeys certain properties. No two exact stock trajectories will look the same, but we may still be able to model both of them using the same equations. In practice, people _generate_ examples of Brownian motion, and/or study specific instantiations of processes modeled by it.
- Brownian motion looks self-similar. Zooming in or out of Brownian motion looks like more Brownian motion, though probably not an exact copy of the original.
- It tends to grow at a rate proportional to $\sqrt{\Delta t}$, where $\Delta t$ is the amount of time that has passed. This also means if we zoom into the time axis by a factor of 4, we only need to zoom into the field axis by a factor of 2 for it to look basically the same, subject to the same caveat as above.
- If you know the value at one point in time, that affects the probability distribution of what values other points can take. We will explore the details of this in just a moment.
- It has what is called a Markov property: if you know the value at a certain point $t$ in time, the probability distributions of value before $t$ become independent of those after $t$, and vice versa.

Ideally, we would be able to state our theory in math, as this will be the most precise way of understanding what we're actually saying and of how our model works. Indeed, one might say that the purpose of math as a language is to exactly describe some idea, model, or theoretical system, where all assumptions and relations must be exactly laid out.

One of the fundamental properties of Brownian motion is the probability relation between values of the field at two points. Specifically, for a function $\phi(t)$ representing Brownian motion, we have the following Probability Density Function (PDF), denoted $\mathcal{P}$, in $\phi_1$:

$\mathcal{P}\left(\phi(t_1)=\phi_1 \mid \phi(t_0)=\phi_0\right) \sim \exp\left(-\tfrac{1}{2}\frac{\left(\phi_1-\phi_0\right)^2}{\nu |t_1-t_0|}\right) = \exp\left(-\frac{1}{2}\frac{\Delta\phi^2}{\nu\Delta t}\right)$.

Most of the fundamental mathematical objects in this article will look like the above, so take your time to get used to it if you need. This is a normal distribution for $\phi_1$ with mean $\phi_0$ and variance $\nu \Delta t$. Here, $\nu$ is a proportionality constant with units of $\phi^2 / t$. The larger the value of $\nu$, the faster the resulting Brownian motion changes field value. Probability density functions also have a proportionality constant so that they integrate to one, but we will be omitting this for clarity. The important thing for our discussion is the exponential and its contents. As such, we only say that the PDF is _proportional_ to the exponential expression, denoted by the $\sim$. Moreover, this proportionality constant is not a function of $\phi_0$ or $\phi_1$, but it may be a function of $\nu$, $t_0$, and $t_1$. From now on, $\sim$ forbids proportionality constants that depend on any $\phi_i$.

## Correlation functions
For concision, I will call the above exponential expression the "correlation function" for this configuration of points, for reasons we will see later. (TODO: Make this a tooltip ->) I do not know if this has an existing name, but I would love know if there is! Note that from a correlation function, the actual PDF can be recovered by integrating the correlation function over all inputs, getting the result, and dividing the correlation function by that result. This means that when we integrate this new quantity over all inputs, we get $1$, which is the requirement on a Probability Density Function.

So we have the correlation function for the value of $\phi(t_1)$ given knowledge of $\phi(t_0)$. And because of the Markov property, if $t_0<t_1<t_2$, then given knowledge of $\phi(t_1)$, the distributions for $\phi(t_0)$ and $\phi(t_2)$ will be independent. This is because knowing the value for $\phi(t_1)$ effectively "cuts off" influence between $t_0$ and $t_2$. As a result, the joint correlation function for $\phi(t_0)$ and $\phi(t_2)$ given $\phi(t_1)$ is just the product of the individual correlation functions. But what if we know $\phi(t_0)$, and we want to get the joint correlation function for $\phi(t_1)$ and $\phi(t_2)$?

So we want to compute $\mathcal{P}\left(\phi(t_1)=\phi_1 \land \phi(t_2)=\phi_2 \mid \phi(t_0)=\phi_0\right)$. By Bayes' theorem, this equals $\mathcal{P}\left(\phi(t_2)=\phi_2 \mid \phi(t_0)=\phi_0 \land \phi(t_1)=\phi_1\right)\mathcal{P}\left(\phi(t_1)=\phi_1 \mid \phi(t_0)=\phi_0\right)$. Applying the Markov property, we simplify to $\mathcal{P}\left(\phi(t_2)=\phi_2 \mid \phi(t_1)=\phi_1\right)\mathcal{P}\left(\phi(t_1)=\phi_1 \mid \phi(t_0)=\phi_0\right)$. Because this is a product of PDFs, the correlation function for this will also just be the product of each individual correlation function. As a result, the correlation function for the joint distribution is $\exp\left(-\tfrac{1}{2}\left(\frac{\left(\phi_2-\phi_1\right)^2}{\nu|t_2-t_1|} + \frac{\left(\phi_1-\phi_0\right)^2}{\nu|t_1-t_0|}\right)\right)$. Notice how within the exponential expression, we simply add the two fractions associated with each individual correlation function. Not only that, it can be expressed as a quadratic polynomial in the $\phi_i$'s, and in general this form looks like it has a lot of structure.

This is very nice! This result is a sign that this kind of problem might continue to yield to symbolic methods.

### Integrating correlation functions
What I'd like to do now is walk through a certain operation on correlation functions. We have the joint correlation function for $\phi(t_1)$ and $\phi(t_2)$ given $\phi(t_0)$, but we'd like to prove that if we are somehow able to ignore the information we get about $\phi(t_1)$, that we recover the correlation function for $\phi(t_2)$ given $\phi(t_0)$. This would basically ensure that our joint correlation function is self-consistent, and is the true generalization of the individual correlation functions.

We will do this by integrating the correlation function over all possible values of $\phi_1$. This is analogous to the probabilistic idea that $P(A \land B) + P(A\land \neg B)=P(A)$. (TODO: Maybe improve this analogy/explanation.) Formally speaking, this looks like $\mathcal{P}\left(\phi(t_2)=\phi_2 \mid \phi(t_0)=\phi_0\right)=\int_\mathbb{R}\mathcal{P}\left(\phi(t_1)=\phi_1\land \phi(t_2)=\phi_2 \mid \phi(t_0)=\phi_0\right)d\phi_1$. (Note the equality because this is written using PDFs.) But because we are working with correlation functions, we instead say $\mathcal{P}\left(\phi(t_2)=\phi_2 \mid \phi(t_0)=\phi_0\right)\sim\int_\mathbb{R} \exp\left(-\tfrac{1}{2}\left(\frac{\left(\phi_2-\phi_1\right)^2}{\nu|t_2-t_1|} + \frac{\left(\phi_1-\phi_0\right)^2}{\nu|t_1-t_0|}\right)\right) d\phi_1$. Now, it turns out that given constants $v_j$ for $j\in\{0\ldots n\}$, $\int_\mathbb{R}\exp\left(-\tfrac{1}{2}\left(\sum_{j=0}^nv_j\phi_j\right)^2\right)d\phi_i\sim 1$. However, if instead the correlation function looks like $\exp\left(-\tfrac{1}{2}\left(\left(\sum_{j=0}^nv_j\phi_j\right)^2+\eta\right)\right)$ where $\eta$ is just some term that doesn't depend on $\phi_i$, then integrating over $\phi_i$ will just result in $\exp\left(-\tfrac{1}{2}\eta\right)$. The intuition is we have isolated all the behavior associated with $\phi_i$ into a self-contained Gaussian distribution for $\phi_i$, represented by a square quadratic in the correlation function, at which point integrating over it eliminates it cleanly.

Ok wow, this seems like an elegant tool. All we have to do is put our correlation function into a form like this, and then we immediately get what the correlation function would be if we integrate over $\phi_1$! It turns out that $\frac{\left(\phi_2-\phi_1\right)^2}{\nu|t_2-t_1|} + \frac{\left(\phi_1-\phi_0\right)^2}{\nu|t_1-t_0|}=\tfrac{1}{\nu}\left(\frac{1}{\left|t_2-t_1\right|}+\frac{1}{\left|t_1-t_0\right|}\right)^{-1}\left(\frac{\phi_1-\phi_2}{\left|t_2-t_1\right|}+\frac{\phi_1-\phi_0}{\left|t_1-t_0\right|}\right)^{2}+\frac{\left(\phi_2-\phi_0\right)^{2}}{\nu\left|t_2-t_0\right|}$. This might look pretty nasty, but the upshot is that the first term in the sum is exactly that square quadratic isolating $\phi_1$ we were looking for. The second term would then be what the correlation function reduces to when we integrate over $\phi_1$, that is to say $\exp\left(-\tfrac{1}{2}\frac{\left(\phi_2-\phi_0\right)^{2}}{\nu\left|t_2-t_0\right|}\right)$. And hey, look! It's exactly what we would expect the correlation function for $\phi(t_2)$ to be given knowledge of $\phi(t_0)$.

### Correlation functions are indifferent to which point is taken as given
Ok, so we know what the correlation function for $\phi(t_1)$ and $\phi(t_2)$ is given $\phi(t_0)$; it's $\exp\left(-\tfrac{1}{2}\left(\frac{\left(\phi_2-\phi_1\right)^2}{\nu|t_2-t_1|} + \frac{\left(\phi_1-\phi_0\right)^2}{\nu|t_1-t_0|}\right)\right)$. What is the correlation function for $\phi(t_0)$ and $\phi(t_1)$ given $\phi(t_2)$? Well, by the above argument the correlation function should be the product of the correlation function for $\phi(t_0)$ given $\phi(t_1)$ times the correlation function for $\phi(t_1)$ given $\phi(t_2)$, which is also $\exp\left(-\tfrac{1}{2}\left(\frac{\left(\phi_2-\phi_1\right)^2}{\nu|t_2-t_1|} + \frac{\left(\phi_1-\phi_0\right)^2}{\nu|t_1-t_0|}\right)\right)$. And the correlation function for $\phi(t_0)$ and $\phi(t_2)$ given $\phi(t_1)$ is the product of those two individual correlation functions, meaning the joint correlation function is ALSO $\exp\left(-\tfrac{1}{2}\left(\frac{\left(\phi_2-\phi_1\right)^2}{\nu|t_2-t_1|} + \frac{\left(\phi_1-\phi_0\right)^2}{\nu|t_1-t_0|}\right)\right)$.

So... the correlation function for $\phi(t_0)$ and $\phi(t_1)$ given $\phi(t_2)$ equals the correlation function for $\phi(t_1)$ and $\phi(t_2)$ given $\phi(t_0)$ equals the correlation function for $\phi(t_0)$ and $\phi(t_2)$ given $\phi(t_1)$. No matter which $\phi(t_i)$ is the "given", they are all the same. In fact, this is a general property of correlation functions. We will prove this in a moment, but we will introduce a shorthand as otherwise the terms involved would be lengthy. Instead of writing something like $\mathcal{P}\left(\phi(t_{i_0})=\phi_{i_0}\land\phi(t_{i_1})=\phi_{i_1}\land\ldots\land\phi(t_{i_m})=\phi_{i_m}\mid\phi(t_{i_{m+1}})=\phi_{i_{m+1}}\land\ldots\land\phi(t_{i_{m'}})=\phi_{i_{m'}}\right)$, I will instead just write out the relevant indices, like $\mathcal{P}\left(i_0,i_1,\ldots,i_m\mid i_{m+1},\ldots,i_{m'}\right)$. Later, we will be generalizing away from time anyways, so this will also come in handy then. (TODO check that we actually use $\mathcal{P}$ later.)

On to the proof. Let $C_i(\phi_0, \ldots, \phi_n)$ be the correlation function for all $\phi(t_j)$ except $\phi(t_i)$, with $\phi(t_i)$ instead being given. We want to prove that $C_i(\phi_0, \ldots, \phi_n)=C_j(\phi_0, \ldots, \phi_n)$ for all $i$ and $j$. By Bayes' rule, $\mathcal{P}\left(0,\ldots,i-1,i+1,\ldots,n\mid i\right)=\mathcal{P}\left(0,\ldots,j-1,j+1,\ldots,n\mid j\right)\frac{\mathcal{P}\left(j\mid i\right)}{\mathcal{P}\left(i\mid j\right)}$. So as long as $\mathcal{P}\left(i\mid j\right)\sim\mathcal{P}\left(j\mid i\right)$, then $\mathcal{P}\left(0,\ldots,i-1,i+1,\ldots,n\mid i\right)\sim\mathcal{P}\left(0,\ldots,j-1,j+1,\ldots,n\mid i\right)$. We can already prove that $C_i(\phi_0, \ldots, \phi_n)\sim C_j(\phi_0, \ldots, \phi_n)$ because of the proportional relationship of each to their PDFs, but we would also like to prove that they are exactly equal. An important thing to note is that because of the constraint on propotionality constants, in the case where each correlation function is the exponential of a polynomial where all terms are exactly degree 2 over $\phi_0,\ldots,\phi_n$, then any kind of "proportionality constant" would look like an extra term of degree 0 in the polynomial, which if nonzero is disallowed by construction. Therefore, the proportionality constant must always be $1$, which means that $C_i(\phi_0, \ldots, \phi_n)=C_j(\phi_0, \ldots, \phi_n)$ in these cases.

What this means is that as long as each of the individual correlation functions $C_i(\phi_i,\phi_j)$ and $C_j(\phi_i,\phi_j)$ are equal, then it is always true that $C_i(\phi_i,\phi_j,\phi_{i_0},\ldots,\phi_{i_n})=C_j(\phi_i,\phi_j,\phi_{i_0},\ldots,\phi_{i_n})$. This is completely true in the case of Brownian motion, where the correlation function in both cases is just $\exp\left(-\tfrac{1}{2}\frac{\left(\phi_j-\phi_i\right)^2}{\nu |t_j-t_i|}\right)$. In other words, we can completely drop the subscript on $C$ and just write $C(\phi_0,\ldots,\phi_n)$.

These correlation functions, then, somehow capture the actual level of mutual likelihood for any configuration of values $\phi(t_0),\ldots,\phi(t_n)$, regardless of which one is being treated as "given". As such, they encode the level of "correlation" between different points in the configuration. They may seem simple right now because of the Markov property making them easily factorable, but they will grow more complex in the later sections, where they will serve as invaluable tools in our analysis.

At this point we should have a pretty good handle on Brownian motion, and we are equipped with correlation functions as a tool to study the joint probability distributions of configurations of points in a Brownian motion process. We are ready for something new.

## Generalizing Brownian motion
Consider that our $\phi$ so far has been a random function $\mathbb{R} \to \mathbb{R}$ that obeys certain statistical properties. But we want random fields! The argument to $\phi$ thus far has meant time, but we are by no means limited to such semantics. We can instead imagine that the $\phi$'s were static fields that fluctuate along any kind of 1-dimensional space. What if our $\phi$ was instead $\mathbb{R}^2 \to \mathbb{R}$ or $\mathbb{R}^3 \to \mathbb{R}$? What about more complex spaces, like the surface of a sphere or a hyperbolic space? How far can we push this thing? And this is an interesting idea, but we still need some kind of guiding principle or intuition to constrain the space of possibilities and give us some interesting mathematical structure that we can analyze.

Before we continue, I need to introduce the idea of a geodesic. A geodesic is basically the shortest path between two points in some space. If we imagine a particle moving in the space, it is the trajectory the particle would take if it is unaffected by external forces. It generalizes the concept of a "straight line" in Euclidean spaces like $\mathbb{R}^n$, and in Euclidean spaces the geodesics are just the straight lines. Walking along a latitude line is a geodesic on the surface of a sphere, but aside from the equator, walking along a longitude line is not because the trajectory is curved. (Do you see why?) We will assume that we are in a metric space, with points in the underlying space represented by $x \in M$, and with a distance function $d:M\times M\to \mathbb{R}$ that obeys [the standard properties of a distance function](https://en.wikipedia.org/wiki/Metric_space#Definition). (TODO do I need to spell out what all the properties are?) We represent a geodesic by a function $\Gamma(t)$ that maps some distance parameter $t$ to points in the underlying space.

What if our random fields $\phi$ obey the following property: along any geodesic $\Gamma(t)$, the function $\phi(\Gamma(t))$ looks like a regular instance of 1D Brownian motion. We can even simplify this to the following condition: the correlation function for two points $\phi(x_0)$ and $\phi(x_1)$ with field strengths $\phi_0$ and $\phi_1$ is $\exp\left(-\tfrac{1}{2}\frac{\left(\phi_1-\phi_0\right)^2}{\nu d(x_0, x_1)}\right)$. It should be obvious to see that the geodesic formulation immediately implies the two-point formulation, but the other way around is actually incorrect, because in some special cases $\Gamma(t)$ can self-intersect in ways that violate the assumptions of Brownian motion. This is fine actually, because the weaker two-point formulation is all we need and also easier to use.

### Solving configurations of three points
Before I drop the solution to the generalized case of $n$ points, let's start with the simplest nontrivial generalization: the correlation function for three points in an arbitrary metric space.

## Possible next steps
- Does this description match any other known formulations of random fields? Completely or only partially?
- Finding the specific correlation functions for complex configurations of points.
    - Finding the joint probability distribution of a set of points evenly distributed on the unit circle, with and without a point in the center as well.
    - Generalizing to the limit of infinite points around the unit circle. Does such a limit even exist? What is the natural form for its correlation function?
    - Generalizing to an arbitrary point inside or outside the unit circle, in addition to the unit circle itself.
    - Generalizing to a unit $n$-sphere in $n+1$ dimensions and the point at the origin.
    - Finding the joint probability distribution of a dense set of points on a line in the plane, and two points that reflect each other through the line on either side of it.
- Proving a variation of the Markov property: given a surface that divides the space into two regions, and knowledge of all values of the function on that surface, the probabilities of points in one region should be independent of points in the other.
- Making an interactive tool by which one can place points on a plane and see the correlation function that arises from that configuration.
- Brownian motion can be generated by "just add or subtract a little bit every timestep". Do these kinds of random fields lend themselves naturally to any kind of similarly simple generation algorithms over Euclidean spaces? Do these methods handle constraints well? Do they generalize to non-Euclidean geometries?
    - If the two endpoints are constrained, can this be used to make cool simulations like lightning that evolve continuously in time?
- Can this theory be extended to geometries that are not completely connected, like graphs?
